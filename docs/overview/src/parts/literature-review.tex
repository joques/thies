\section{Literature Review}
\label{sec:Literature Review} % (fold)

\subsection{Sequential Decion-Making}
\label{sub:lit-rev_sequential-decion} % (fold)

\Cite{slivkins:2024} discusses the \glspl{mab}  problem and its relevance to the research topic. It introduces the concept and discusses various algorithms to achieve decision-making under uncertainty. The term \emph{multi-armed bandit} includes an area where statistics, probability, and cutting-edge algorithms come together to assist in making calculated choices and maximizing their endeavors. The \gls{mab} is a simple yet very powerful framework for algorithms that manage uncertainty while making decisions over time. 

An agent must select actions (or arms) in this machine learning setting in order to maximize its cumulative long-term payoff~\cite{zhao:2025}. The agent is given some context about the current condition at the beginning of each round. It makes its decision based on this data as well as the knowledge gathered from earlier rounds. The reward linked to the chosen action is given to the agent at the conclusion of each round.

In the MAB problem, an agent is given several options (or "arms"), each of which offers a reward selected from an unknown probability distribution. The agent seeks to maximize its cumulative reward across a sequence of trials (Vermorel and Mohri, 2005). In order to choose the best arm, one must strike a balance between exploring different arms to learn more about their reward distribution and taking use of recognized arms that have historically yielded large payouts.

% subsection Sequential Decion-Making (end)

\subsection{Decentralised Partially Observable Markov Decision Processes}
\label{sub:lit-rev_dec-pomdp}

Ref not found
This study outlines the decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework and its relevance to the research topic: "building a multi-agent system for sorting recyclable waste at refuse stations".


A group of agents collaborates to optimize a global reward using only local information in a decentralized partially observable Markov decision process (Dec-POMDP). As a result, the agents do not perceive a Markovian signal during their execution, which means their specific policies are derived from their histories and translated into actions. This paper reviews recent research on decentralized Markov decision processes (MDPs), where each agent's control is influenced by a limited perspective. We focus on a general framework represented as a Dec-POMDP, which accounts for uncertainty regarding the environmental state.


Previous studies have broadened the scope of decision-making to include multiple agents and to operate in situations where the state is unknown, such as in Partially Observable Markov Decision Processes (POMDPs). Additionally, most research on multi-agent systems under partial observability has primarily focused on planning scenarios where the environmental model is already provided, rather than on complete reinforcement learning (RL) settings. In this study, we aim to further generalize this research by considering scenarios with multiple agents and state uncertainty. Specifically, it focuses on groups of cooperative agents that share a common goal. The Dec-POMDPs can be used to define such circumstances (Pynadath and Tambe, 2002; Bernstein et al, 2002).  Additionally, the research addresses issues about sequential decision-making under Dec-POMDPS and concentrates solely on planning, with some recommendations for RL techniques. Lastly, the paper presents the Dec-POMDPS planning model and pertinent Dec-POMDPS scenarios.

This model is based on the principle that, in a scenario involving two agents, each agent acts independently at every stage. The state of the environment changes depending on the actions taken by both agents, resulting in a reward. Each agent receives a unique observation of the new state. There is a common understanding that planning occurs offline, with the plans then executed online. During the fully decentralized online phase, each agent has access to its own history of actions and observations, as well as its share of the joint policy developed during the planning phase (Chang & Kaelbling, 2003). In contrast, the offline planning stage is centralized. According to Claus and Boutilier (1998), we assume that a single computer calculates the joint plan and subsequently distributes it to the agents, who then carry it out online.

We provide a concise overview of recent developments relevant to addressing the Dec-POMDP (Decentralized Partially Observable Markov Decision Process) challenge. In part II, we will explicitly discuss sample scenarios of the Dec-POMDP model. Next, in section III, we will explore Dec-POMDP planning, including actions, states, and observations. Sections IV and V present insights on sequential decision-making within the context of Dec-POMDP and reinforcement learning (RL) respectively. In section VI, we will introduce several notable classes of Dec-POMDPs. Finally, in part VII, we will conclude our discussion.



% section Literature Review (end)
